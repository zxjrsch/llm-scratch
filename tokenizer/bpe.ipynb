{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6802ece2",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenizer\n",
    "\n",
    "Unicode is a mapping that assigns to every character an integer called the code point of that character. This scheme for converting character to numbers is called **character level tokenizaiton**. For concreteness, Unicode currently encodes slightly under 0.2 million characters. \n",
    "\n",
    "Since the number of all human characters is large, this one-to-one tokenization leads to a huge vocabulary. One way of reducing the vocabulary size is to group together bytes and leverage **byte level tokenization** by adopt an encoding. For example in the dominant encoding for text on the internet, the UTF-8 encoding, every code point is represented by a seqence of at most four bytes. Each byte encodes an integers in [0, 256) hence the vocabulary size of the tokens is 256. One can also adopt UTF-16 encoding, so as to use a vocabulary of size $2^{16}$. Observe the tradeoff between vocabulary size and the length of tokenized text with respect to that vocabulary. \n",
    "\n",
    "The idea of **subword tokenization** is to create tokens out of groups of characters in a word. One proposal of a method for identifying such subwords to be considered a token is called **byte-pair encoding**, which mints new tokens from the most freqently occuring pair of bytes. Thus freqently occuring sets of characters are considered units called tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a570e97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ðŸ˜‚'.isidentifier(), 'Ï€'.isidentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23199eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ï€'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single character\n",
    "Ï€ = ord('Ï€')  # code point\n",
    "chr(Ï€) # actual char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87f77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz"
     ]
    }
   ],
   "source": [
    "for n in range(65, 123):\n",
    "    print(chr(n), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e69834e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'> b'\\xf0\\x9f\\x99\\x82' [240, 159, 153, 130] ðŸ™‚\n"
     ]
    }
   ],
   "source": [
    "s = 'ðŸ™‚'\n",
    "utf8_s = s.encode('utf-8')\n",
    "print(type(utf8_s), utf8_s, list(utf8_s), utf8_s.decode('utf-8'))   # observe four bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a4cd2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat|?| cat|!| cat|,| cat|.| I|'m| a| byte|-pair| tokenizer|3|.|141|592|6|"
     ]
    }
   ],
   "source": [
    "import regex\n",
    "\n",
    "# pre-tokenization pattern\n",
    "PAT = r\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\n",
    "\n",
    "corpus = r\"\"\"cat? cat! cat, cat. I'm a byte-pair tokenizer3.1415926\"\"\"\n",
    "\n",
    "itr = regex.finditer(PAT, corpus)\n",
    "\n",
    "for item in itr:\n",
    "    print(item.group(), end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66a6c59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = r\"\"\"\n",
    "All the world's a stage,\n",
    "And all the men and women merely players;\n",
    "They have their exits and their entrances;\n",
    "And one man in his time plays many parts,\n",
    "His acts being seven ages. At first the infant,\n",
    "Mewling and puking in the nurse's arms;\n",
    "And then the whining school-boy, with his satchel\n",
    "And shining morning face, creeping like snail\n",
    "Unwillingly to school. And then the lover,\n",
    "Sighing like furnace, with a woeful ballad\n",
    "Made to his mistress' eyebrow. Then a soldier,\n",
    "Full of strange oaths, and bearded like the pard,\n",
    "Jealous in honour, sudden and quick in quarrel,\n",
    "Seeking the bubble reputation\n",
    "Even in the cannon's mouth. And then the justice,\n",
    "In fair round belly with good capon lin'd,\n",
    "With eyes severe and beard of formal cut,\n",
    "Full of wise saws and modern instances;\n",
    "And so he plays his part. The sixth age shifts\n",
    "Into the lean and slipper'd pantaloon,\n",
    "With spectacles on nose and pouch on side;\n",
    "His youthful hose, well sav'd, a world too wide\n",
    "For his shrunk shank; and his big manly voice,\n",
    "Turning again toward childish treble, pipes\n",
    "And whistles in his sound. Last scene of all,\n",
    "That ends this strange eventful history,\n",
    "Is second childishness and mere oblivion;\n",
    "Sans teeth, sans eyes, sans taste, sans everything.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6d8958",
   "metadata": {},
   "source": [
    "# Training a BPE Tokenizer\n",
    "\n",
    "At the start we know that all 256 possible bytes will be a subset of the final token vocabulary.\n",
    "\n",
    "We need to pre-tokenize in order to treat semantically close words like `cat,` and `cat.` and `cat!` similarly.\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. [Python Unicode Documenation](https://docs.python.org/3/howto/unicode.html)\n",
    "2. [GPT 2 tokenizer (paper)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fce50283",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# initialize\n",
    "vocab = list(range(256))\n",
    "\n",
    "corpus_utf8 = list(corpus.encode('utf-8'))\n",
    "byte_pairs = Counter(zip(corpus_utf8, corpus_utf8[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8a67bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the world's a stage,\n",
      "And all the men and women merely players;\n",
      "They have their exits and their entrances;\n",
      "And one man in his time plays many part\n"
     ]
    }
   ],
   "source": [
    "# # Approach 1\n",
    "# s = ''\n",
    "# for i in list(corpus.encode('utf-8')):\n",
    "#     s += chr(i)\n",
    "# print(s)\n",
    "\n",
    "# # Approach 2\n",
    "decoded_text = bytes(corpus_utf8).decode('utf-8')\n",
    "print(decoded_text[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0f720b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121 â‰¤ 256\n",
      "1222 chars [10, 65, 108, 108, 32, 116, 104, 101, 32, 119]\n",
      "313 pairs Counter({(101, 32): 29, (115, 32): 29, (116, 104), ...)\n"
     ]
    }
   ],
   "source": [
    "print(max(corpus_utf8), 'â‰¤', len(vocab))\n",
    "print(len(corpus_utf8), 'chars', corpus_utf8[:10])\n",
    "print(len(byte_pairs), 'pairs', 'Counter({(101, 32): 29, (115, 32): 29, (116, 104), ...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97276b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(e| ) pair count: 29\n",
      "(s| ) pair count: 29\n",
      "(t|h) pair count: 28\n",
      "( |s) pair count: 28\n",
      "(d| ) pair count: 27\n"
     ]
    }
   ],
   "source": [
    "for (a, b), freq in byte_pairs.most_common(5):\n",
    "    print(f'({chr(a)}|{chr(b)}) pair count: {freq}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac50b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "\n",
    "def merge_pair(original_corpus: List[int], p1: int, p2: int, replacement_token: int) -> List[int]:\n",
    "    L = len(original_corpus)\n",
    "    if L < 2:\n",
    "        return original_corpus\n",
    "    merged_corpus = []\n",
    "    i = 0\n",
    "    while i < L:\n",
    "        # or use replace() method if cropus is str\n",
    "        if i < L-1 and original_corpus[i: i+2] == [p1, p2]:\n",
    "            merged_corpus.append(replacement_token)\n",
    "            i +=2\n",
    "        else:\n",
    "            merged_corpus.append(original_corpus[i])\n",
    "            i += 1\n",
    "    return merged_corpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "816003b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['x', 3, 4, 5]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_pair([1, 2, 3, 4, 5], 1, 2, 'x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3440ddcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 [10, 65, 108, 108, 32, 116, 104, 256, 119, 111]\n"
     ]
    }
   ],
   "source": [
    "original_corpus = corpus_utf8\n",
    "(p1, p2), freq = byte_pairs.most_common(1)[0]\n",
    "replacement_token = vocab[-1] + 1\n",
    "vocab.append(replacement_token)\n",
    "\n",
    "merged_corpus = merge_pair(original_corpus, p1, p2, replacement_token)\n",
    "print(max(merged_corpus), merged_corpus[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58b1ea47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1222 = 1193 + 29 (True)\n"
     ]
    }
   ],
   "source": [
    "assert (p1, p2) not in list(zip(merged_corpus, merged_corpus[1:]))\n",
    "print(f'{len(original_corpus)} = {len(merged_corpus)} + {freq} ({len(original_corpus) == len(merged_corpus) + freq})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aac7e880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# naive, and without pretokenization\n",
    "def train_bpe(num_steps: int, corpus: str):\n",
    "\n",
    "    initial_vocab_size = 256\n",
    "    vocab = list(range(initial_vocab_size))\n",
    "    corpus = list(corpus.encode('utf-8'))\n",
    "    vocab_table = {i: i for i in vocab} # vocab: byte_pair\n",
    "    vocab_table = {}\n",
    "    \n",
    "    for step in range(num_steps):\n",
    "\n",
    "        # find most common byte pair\n",
    "        pairs = Counter(zip(corpus, corpus[1:]))\n",
    "        (p1, p2), freq = pairs.most_common(1)[0]\n",
    "\n",
    "        if freq == 1:\n",
    "            raise Exception('Frequency 1 met')\n",
    "\n",
    "        # handle ties \n",
    "        tied_pairs = []\n",
    "        for pair, f in pairs.items():\n",
    "            if f == freq:\n",
    "                tied_pairs.append(pair)\n",
    "        p1, p2 = max(tied_pairs)\n",
    "\n",
    "        # record newly created token\n",
    "        replacement_token = vocab[-1]+1\n",
    "        vocab.append(replacement_token)\n",
    "        vocab_table[replacement_token] = (p1, p2)\n",
    "\n",
    "        new_corpus = merge_pair(corpus, p1, p2, replacement_token)\n",
    "        assert len(new_corpus) + freq == len(corpus)\n",
    "        corpus = new_corpus\n",
    "        # print(f'At step {step} merged {freq} instances of {(p1, p2)} as new token {replacement_token}')\n",
    "\n",
    "    return vocab_table, corpus\n",
    "\n",
    "vocab_size = 300\n",
    "initial_vocab_size = 256\n",
    "num_steps = vocab_size - 256\n",
    "vocab, tokenized_corpus = train_bpe(num_steps, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "886570ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(256, (115, 32)),\n",
       " (257, (101, 32)),\n",
       " (258, (116, 104)),\n",
       " (259, (100, 32)),\n",
       " (260, (97, 110)),\n",
       " (261, (105, 110)),\n",
       " (262, (32, 115)),\n",
       " (263, (44, 10)),\n",
       " (264, (110, 32)),\n",
       " (265, (261, 103))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokens representing merges\n",
    "# in addition to the initial tokens 0-255\n",
    "list(vocab.items())[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a4866c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.57"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compression_ratio = len(corpus) / len(tokenized_corpus)\n",
    "compression_ratio = round(compression_ratio, 2)\n",
    "compression_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "46f10797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{256: 's ',\n",
       " 257: 'e ',\n",
       " 258: 'th',\n",
       " 259: 'd ',\n",
       " 260: 'an',\n",
       " 261: 'in',\n",
       " 262: ' s',\n",
       " 263: ',\\n',\n",
       " 264: 'n ',\n",
       " 265: 'ing',\n",
       " 266: 'hi'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_2_str = {i: chr(i) for i in range(initial_vocab_size)}\n",
    "for token, (a, b) in vocab.items():\n",
    "    token_2_str[token] = token_2_str[a] + token_2_str[b] # addition of str\n",
    "\n",
    "# merges\n",
    "limit = initial_vocab_size + 10\n",
    "{token: string for (token, string) in token_2_str.items() if initial_vocab_size <= token <= limit}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bb5445",
   "metadata": {},
   "source": [
    "# Encoding and Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13b56827",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens: List[int], token_to_string: Dict[int, str]) -> str:\n",
    "    s = ''\n",
    "    for t in tokens:\n",
    "        s += token_to_string[t]\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "565382a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 776, 299)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenized_corpus), len(tokenized_corpus), max(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3919ae84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All the world's a stage,\n",
      "And all the men and women merely players;\n",
      "They have their exits and their entrances;\n",
      "And one man in his time plays many part\n"
     ]
    }
   ],
   "source": [
    "decoded_text = decode(tokenized_corpus, token_2_str) \n",
    "assert decoded_text == corpus\n",
    "print(decoded_text[:150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e596a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "104 h\n",
      "101 e\n",
      "271 ll\n",
      "111 o\n",
      "277  w\n",
      "283 or\n",
      "299 ld\n"
     ]
    }
   ],
   "source": [
    "def encode(corpus: str, pairs_to_token: Dict[Tuple[int, int], int]) -> List[int]:\n",
    "    corpus_utf8 = list(corpus.encode('utf-8'))\n",
    "\n",
    "    if len(corpus_utf8) == 1:\n",
    "        return corpus_utf8\n",
    "    \n",
    "    pairs = list(zip(corpus_utf8, corpus_utf8[1:]))\n",
    "    working_pairs = []\n",
    "\n",
    "    while True:\n",
    "        merge_exists = False\n",
    "        i = 0\n",
    "        while i < len(pairs):\n",
    "            \n",
    "            p = pairs[i]\n",
    "            \n",
    "            if p in pairs_to_token:\n",
    "                token = pairs_to_token[p]\n",
    "                working_pairs.append(token)\n",
    "                merge_exists = True # some merging occured\n",
    "                if i + 1 == len(pairs) - 1:\n",
    "                    _, b = pairs[i+1]\n",
    "                    working_pairs.append(b)\n",
    "                    break\n",
    "                i += 2\n",
    "            else:\n",
    "                if i == len(pairs)-1:\n",
    "                    working_pairs += p\n",
    "                    i += 2\n",
    "                else:\n",
    "                    working_pairs.append(p[0])\n",
    "                    i += 1\n",
    "\n",
    "        if not merge_exists or len(working_pairs) == 1:\n",
    "            break\n",
    "        pairs = list(zip(working_pairs, working_pairs[1:]))\n",
    "        if len(pairs) == 1:\n",
    "            break\n",
    "        working_pairs = []\n",
    "    return working_pairs\n",
    "\n",
    "txt = 'hello world'\n",
    "pairs_2_token = {pair: token for (token, pair) in vocab.items()}\n",
    "for i in encode(txt, pairs_2_token):\n",
    "    print(i, token_2_str[i])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
