{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6802ece2",
   "metadata": {},
   "source": [
    "# Byte Pair Encoding Tokenizer\n",
    "\n",
    "Unicode is a mapping that assigns to every character an integer called the code point of that character. This scheme for converting character to numbers is called **character level tokenizaiton**. \n",
    "\n",
    "Since the number of all human characters is large, this one-to-one tokenization leads to a huge vocabulary. One way of reducing the vocabulary size is to group together bytes and leverage **byte level tokenization** by adopt an encoding. For example in UTF-8 encoding, every code point is represented by a seqence of bytes and the possible tokens are integers in [0, 256). One can also adopt UTF-16 encoding, so as to use a vocabulary of size $2^{16}$. Observe the tradeoff between vocabulary size and the length of tokenized text with respect to that vocabulary. \n",
    "\n",
    "The idea of **subword tokenization** is to create tokens out of groups of characters in a word. One proposal of a method for identifying such subwords to be considered a token is called **byte-pair encoding**, which mints new tokens from the most freqently occuring pair of bytes. Thus freqently occuring sets of characters are considered units called tokens.\n",
    "\n",
    "# Training a BPE Tokenizer\n",
    "\n",
    "At the start we know that all 256 possible bytes will be a subset of the final token vocabulary.\n",
    "\n",
    "We need to pre-tokenize in order to treat semantically close words like `cat,` and `cat.` and `cat!` similarly.\n",
    "\n",
    "\n",
    "\n",
    "### References\n",
    "\n",
    "1. [Python Unicode Documenation](https://docs.python.org/3/howto/unicode.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a570e97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'ðŸ˜‚'.isidentifier(), 'Ï€'.isidentifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23199eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ï€'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# single character\n",
    "Ï€ = ord('Ï€')  # code point\n",
    "chr(Ï€) # actual char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e87f77f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_`abcdefghijklmnopqrstuvwxyz"
     ]
    }
   ],
   "source": [
    "for n in range(65, 123):\n",
    "    print(chr(n), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e69834e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'> b'\\xf0\\x9f\\x99\\x82' [240, 159, 153, 130] ðŸ™‚\n"
     ]
    }
   ],
   "source": [
    "s = 'ðŸ™‚'\n",
    "utf8_s = s.encode('utf-8')\n",
    "print(type(utf8_s), utf8_s, list(utf8_s), utf8_s.decode('utf-8'))   # observe four bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "0a4cd2c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I|'m| a| byte|-pair| tokenizer| |123|456|789|012|"
     ]
    }
   ],
   "source": [
    "# pre-tokenization pattern\n",
    "import regex\n",
    "\n",
    "PAT = r\"'(?i:[sdmt]|ll|ve|re)|[^\\r\\n\\p{L}\\p{N}]?+\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]++[\\r\\n]*|\\s*[\\r\\n]|\\s+(?!\\S)|\\s+\"\n",
    "corpus = r\"\"\"I'm a byte-pair tokenizer 123456789012\"\"\"\n",
    "itr = regex.finditer(PAT, corpus)\n",
    "for item in itr:\n",
    "    print(item.group(), end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d59c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
